# 1.11.0 Release ðŸŽ‰

{% note noteType="Tip" %}
**3rd December 2025**
{% /note %}

You can find the GitHub release [here](https://github.com/open-metadata/OpenMetadata/releases/tag/1.11.0-release).

## Features

### Ask Collate

AskCollate is our AI-powered conversational agent that gives you natural-language access to your data catalog and analytics. It combines metadata intelligence with direct data access, helping you discover, understand, analyze, and manage your data assets through simple, intuitive conversations.

#### Key capabilities

Semantic Search: Instantly locate data assets using natural-language queries with advanced filters such as owner, domain, tag, certification, tier, and more.

Metadata Updates: Enrich your catalog by updating descriptions, owners, tags, and other metadata directly through conversational commands.

Text-to-SQL Analysis: Convert plain-English questions into complex SQL queries to retrieve and analyze real-time data from your warehouse (read-only).

Instant Visualizations: Automatically transform SQL results into interactive bar, line, pie, and area charts---right inside the chat.

Lineage & Impact: Explore upstream and downstream dependencies to visualize data flow and assess the potential impact of changes.

Data Quality RCA: Diagnose data-quality issues and trace how test failures propagate across your pipelines.

Data Quality Planner: Generate data quality tests instantaneously using context from your metadata and the entity descriptive statistics

Business Glossary Management: Build and maintain a unified data vocabulary by defining terms, synonyms, and hierarchical relationships through chat.

### Ask Collate Slack Integration

You can now use AskCollate directly within Slack! This integration brings the full power of AskCollate to the place where your teams already collaborate, making it easier than ever to ask questions and access data context the moment you need it.

Simply mention @AskCollate in any channel or thread to tap into your data instantly.

### SQL Studio
Collate has always been the central hub to discover and understand your data, track transformations and lineage, profiling, and data quality, all in one place.

With 1.11, we're taking this a step further by giving your users the ability to query and analyze data directly within Collate.

SQL Studio lets users create personal connections to existing services and start running SQL queries while exploring the catalog.

As a service owner, you maintain full control over which services are available in SQL Studio and how users authenticate---whether through SSO, OAuth, or basic credentials.

Version 1.11 introduces support for Snowflake, BigQuery, and Trino, with additional engines coming in future releases.

### Data Quality as Code

Define and execute data quality tests programmatically using Python, with results automatically published to OpenMetadata's Data Observability dashboard. Integrate validation directly into your transformation pipelines with circuit breaker patterns to prevent bad data from reaching production tables.

Two validation approaches:

TestRunner --- Validate data already loaded in tables. Reference any table by its fully qualified name (FQN), add any test cases, then execute tests and automatically push results back to OpenMetadata. Ideal for post-load validation and monitoring.

DataFrameValidator --- Validate data inline during ETL pipelines. Acts as a circuit breaker to prevent bad data from being loaded. Define on_success and on_failure callbacks to control pipeline behavior. Supports chunked processing for large datasets. Optionally publish results to a specific table in OpenMetadata.

What happens when tests run:

Results automatically sync to OpenMetadata's Data Quality tab. Failed tests generate incidents with detailed failure reasons (e.g., "minimum value below zero: -521"). Incident alert icons appear on affected tables. The Overview tab shows passing and failing dimensions at a glance.

Why use it:

Centralize data quality test logic in version control tools. Provide reusable validation libraries for data engineers. Execute tests as part of your pipeline, not as a separate process. Maintain visibility in Collate while managing logic externally.

### Data Quality Dimensionality

You can now create dimension-level data quality tests that automatically segment results by categorical column values, providing granular visibility into data quality across different data segments.

## What's New

When adding a test case in the Data Observability tab, select "Dimension Level" to associate one or more dimension columns (e.g., status, region, product category) with your column-level test. The new configuration panel provides guidance on use cases and allows you to select multiple dimensions for multi-dimensional analysis.

Test cases with dimensions display a badge indicator showing the number of associated dimensions, making them easy to identify in your test case list.

A new "Dimensionality" tab in test results provides:

- A calendar heatmap showing pass/fail status across all dimension values over time

- A summary table with status, impact score, dimension value, and last run timestamp for each segment

- Drill-down capability to view historical execution trends for any specific dimension value

Impact scores help you quickly identify which dimension values are contributing the most failures based on affected row counts.

Why It Matters:

Previously, testing data quality across different categorical segments required writing custom SQL queries (e.g., SELECT ... WHERE status = 'completed') and maintaining separate test cases for each known value. When new dimension values appeared in your data, you had to manually create additional tests. With dimension-level test cases, a single test automatically covers all current and future values in your dimension column, eliminating maintenance overhead while providing deeper insights into where data quality issues occur.

### Notification Templates

You can now fully customize the content and format of alert notifications delivered to Slack, Microsoft Teams, Google Chat, or email.

Manage templates globally or per-alert

Navigate to Settings â†’ Notifications â†’ Templates to view and manage all notification templates. From here you can edit system default templates (which apply globally) or create new reusable templates available across your organization.

Build dynamic templates with placeholders

Use double curly brace syntax to insert dynamic fields that automatically populate when alerts trigger:

- {{entity.name}} -- Entity display name

- {{entity.owners}} -- Entity owners

- {{entity.description}} -- Entity description

- {{entity.href}} -- Direct link to the entity

- And more

Add conditional logic and rich formatting

Templates support conditional formatting with {{#if}}, {{else}}, and {{/if}} blocks. The rich text editor lets you add bold, italic, code blocks, images, links to assets, and other formatting.

Validate before saving

Click Validate to check your template syntax before saving. Invalid placeholders or syntax errors are highlighted so you can fix them before deployment.

Flexible template assignment

When creating an alert, choose to use the system default template, select a custom template from your organization's library, or create a new template specific to that alert. You can also configure alerts to notify downstream asset owners with customizable depth settings.

## Breaking Changes

### Elasticsearch & Opensearch Version Changes

1. Elasticsearch server version: Verify whether the server is running version 8.x. If it is running an earlier version, please upgrade to 8.x before proceeding.
2. OpenSearch server version: Verify whether the server is running version 2.x. If it is running an earlier version, please upgrade to 2.x before proceeding.

### MySQL Configuration Required for Airflow 3.x Migration

If you are using MySQL as your Airflow metadata database and upgrading to Airflow 3.x (the new default in OpenMetadata 1.11), you must configure MySQL to allow temporary stored function creation during the migration process.

#### Root Cause

During the Airflow 3.x database migration on MySQL, Airflow needs to create a temporary stored function (`uuid_generate_v7`) to backfill UUIDs for the `task_instance` table. When MySQL runs with binary logging enabled (which is the default in most production setups), it blocks function creation unless `log_bin_trust_function_creators` is enabled or the user has SUPER privileges. Without this configuration, the migration fails with an error like:

```
FUNCTION airflow_db.uuid_generate_v7 does not exist
```

This is a known limitation when running Airflow 3.x migrations on MySQL with binary logging enabled. PostgreSQL users are not affected by this issue.

For more details, see the Apache Airflow issues:
- [https://github.com/apache/airflow/issues/49611](https://github.com/apache/airflow/issues/49611)
- [https://github.com/apache/airflow/issues/54554](https://github.com/apache/airflow/issues/54554)

#### Resolution

**Option 1: Delete and Recreate the Airflow Database (Strongly Recommended)**

The simplest and most reliable solution is to delete the existing Airflow database and let OpenMetadata recreate it fresh during startup. The Airflow database only stores workflow execution history and metadataâ€”it does not contain any of your OpenMetadata configurations, connections, or ingestion pipeline definitions.

{% note noteType="Tip" %}
This is the recommended approach because it avoids all migration complexities and ensures a clean state. Your ingestion pipelines and their configurations are stored in the OpenMetadata database, not in Airflow's database.
{% /note %}

```bash
# Connect to your MySQL instance and drop the Airflow database
docker exec -i openmetadata_mysql mysql -u USERNAME -pPASSWORD -e "DROP DATABASE IF EXISTS airflow_db;"
```

Then recreate the database with the proper character set and grant privileges:

```sql
CREATE DATABASE airflow_db CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
GRANT ALL PRIVILEGES ON airflow_db.* TO 'airflow_user'@'%' WITH GRANT OPTION;
FLUSH PRIVILEGES;
```

Execute this via command line:

```bash
# Recreate the Airflow database
docker exec -i openmetadata_mysql mysql -u USERNAME -pPASSWORD -e "CREATE DATABASE airflow_db CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; GRANT ALL PRIVILEGES ON airflow_db.* TO 'airflow_user'@'%' WITH GRANT OPTION; FLUSH PRIVILEGES;"

# Restart the ingestion container to run migrations on the fresh database
docker restart openmetadata_ingestion
```

{% note noteType="Warning" %}
Replace `USERNAME` and `PASSWORD` with your MySQL root credentials, and `airflow_user` with your actual Airflow database user if different. For Docker Quickstart deployments, the default root credentials are `root` / `password`.
{% /note %}

{% note noteType="Important" %}
After the ingestion container restarts successfully, you must **redeploy all your ingestion pipelines** from the OpenMetadata UI. This registers the DAGs in the fresh Airflow database.
{% /note %}

---

**Option 2: Manual Migration Fix (If You Cannot Delete the Database)**

If you have specific requirements to preserve the Airflow execution history and cannot delete the database, follow the manual steps below.

**Step 1: Enable MySQL Configuration**

First, enable `log_bin_trust_function_creators` in your MySQL instance to allow Airflow to create the necessary stored function:

For Docker deployments, add this to your `docker-compose.yml` file under the MySQL service:

```yaml
services:
  mysql:
    command: "--log-bin-trust-function-creators=1"
```

For standalone MySQL instances, execute this query as a user with sufficient privileges:

```sql
SET GLOBAL log_bin_trust_function_creators = 1;
```

**Step 2: Clean Airflow Database**

After enabling the MySQL configuration, choose one of the following options based on your situation:

**Option 2a: Truncate Task Instance Table**

If you want to avoid conflicting migration changes, you can truncate the `task_instance` table. This approach removes all task execution history but preserves your DAGs and connections.

{% note noteType="Warning" %}
This will delete all historical task execution data. Only use this if you're okay with losing task run history.
{% /note %}

```sql
-- Clean task_instance table to avoid migration conflicts
USE airflow_db;

-- Truncate task_instance table
TRUNCATE TABLE task_instance;

-- Verify the table is empty
SELECT COUNT(*) FROM task_instance;
```

Execute this script:

```bash
# Run the cleanup script on your MySQL container
docker exec -i openmetadata_mysql mysql -u USERNAME -pPASSWORD -e "USE airflow_db; TRUNCATE TABLE task_instance; SELECT COUNT(*) as remaining_rows FROM task_instance;"

# Restart the ingestion container to apply migrations
docker restart openmetadata_ingestion
```

**Option 2b: Fix Stuck Migrations (If Migration Already Failed)**

If your migration is already stuck midway (the `task_instance` table was partially modified), you need to reset the migration state before restarting. Save the following SQL script as `fix_airflow_migration.sql`:

```sql
-- Fix Airflow 3.x migration issue
-- This script fixes the partial migration of task_instance table

USE airflow_db;

-- Check if the migration was partially applied
-- If 'id' column exists but isn't properly configured, we need to fix it

-- First, check the current state
SHOW COLUMNS FROM task_instance LIKE 'id';

-- Drop the problematic column if it exists
SET @exist := (SELECT COUNT(*) FROM information_schema.COLUMNS
    WHERE TABLE_SCHEMA = 'airflow_db'
    AND TABLE_NAME = 'task_instance'
    AND COLUMN_NAME = 'id');

SET @sqlstmt := IF(@exist > 0,
    'ALTER TABLE task_instance DROP COLUMN id',
    'SELECT ''Column does not exist'' AS status');

PREPARE stmt FROM @sqlstmt;
EXECUTE stmt;
DEALLOCATE PREPARE stmt;

-- Reset the alembic version to before this migration
-- The migration that's failing is: d59cbbef95eb (Add UUID primary key to task_instance)
-- We need to set it back to the previous version: 05234396c6fc
UPDATE alembic_version SET version_num = '05234396c6fc' WHERE version_num = 'd59cbbef95eb';

-- Verify the changes
SELECT * FROM alembic_version;
SHOW COLUMNS FROM task_instance LIKE 'id';
```

Then execute the script and restart the container:

```bash
# Run the fix script on your MySQL container
docker exec -i openmetadata_mysql mysql -u USERNAME -pPASSWORD < fix_airflow_migration.sql

# Restart the ingestion container
docker restart openmetadata_ingestion
```

{% note noteType="Warning" %}
Replace `USERNAME` and `PASSWORD` with your actual MySQL credentials, and ensure the database name matches your configuration (default is `airflow_db`).
{% /note %}

### Collate - Deprecating Metapilot

We are removing the Metapilot UI in favor of the new Ask Collate features. We are not removing any functionality, but enhancing both the chat experience and query optimization agents.

- Instead of the floating Metapilot icon on the corner of the UI, now we have a dedicated AskCollate interface you can access from the navigation panel. There, you can interact with AskCollate to answer your questions both on data and metadata.

- The chat experience when analyzing queries in the Queries tab of table assets will now also use the new SQL Agent, while maintaining the existing experience.

### Deprecating Python 3.9

Python 3.9 became EOL in October 2025, and most of the Ingestion Framework dependencies already dropped its support.

We are now removing support for Python 3.9 in the ingestion framework and adding support for Python 3.12.

### Airflow 3.X will be the new default

As part of the python changes, we're also updating the default OSS ingestion image to be based on Airflow 3.X.

If you are still using 2.10.X in your own custom images, we will still support that version.

### POST `api/v1/dataQuality/testCases` Permission Change

We previously enforced the `EditTests` operations on the `Table` resource for creating test case permission. We have now introduced a new `CreateTests` operation on the `Table` resource for finer grain permission control over create vs edit tests for Table entities.

If you previously had an `EditTests` operation for a `Table` resource on your policy meant to prevent creation of test cases you will need to add the `CreateTests` operation as part of your policy.

## Changelog

### Fixes

- Security & dependencies: multiple vulnerability fixes and dependency updates (netty, commons-lang3, org.json, angus mail, protobuf) and pinned pydantic to <2.12.0 to address compatibility issues.
- UI fixes: resolved numerous UI bugs including import warning messages, left-sidebar settings icon, drawer loading overlay scroll behavior, Data Product icon stroke width, default font sizing and color, tab rendering and persona page issues, duplicate owners/tier fields, activity feed header/navigation and widget title widths, console warnings, lineage paging/rendering, edit-lineage button placement, description rendering on non-Chromium browsers, and other component errors and warnings.
- Search & indexing: fixes for vector search index creation/validation, prevention of truncation in Elasticsearch indices (added explicit fqnHash and lineage mappings with ignore_above 512), restored missing search documents for incidents, and addressed large-parameter and indexing edge cases.
- Query Runner & DB fixes: enabled parallel execution for Query Runner; DB query fixes (SSO/OAuth); SELECT-only enforcement; saved-query per user; pagination added for Snowflake usage/lineage queries; various Snowflake/BigQuery CLI and query-related bug fixes.
- Data quality & dimensionality: fixes to Data Quality dashboard filtering, freshness tests, dimensionality validators, incident reporting and consolidated ChangeEvents for test runs; migrations and retention fixes for test case results and profile data.
- Connectors & lineage parsing: Databricks/DLT auth and parsing fixes, PowerBI native query lineage extraction and custom API URL fixes, Kafka Connect lineage and Confluent Cloud parsing fixes, BigQuery exporter bug fixes, Collibra connector fixes.
- API & backend fixes: domain assets count mismatch, glossary term circular/self-referential parent handling (preventing API hangs), table diff validation parameter (table2.keyColumns), support for datamodel source URL, explicit fixes for view-name scoping and local-variable issues, and prevention of double notifications on cover image upload failures.
- Authentication, tokens & SSO: LDAP login retry improvements, SAML/Azure AD timestamp compatibility, Support App token evaluation fixes, improved bot/OIDC handling, and explicit bearer-token error messaging.
- Migrations & reliability: migration fixes and adjustments for multiple versions (including 1.10.x â†’ 1.11.x moves), zero-downtime reindexing orphan cleanup, socket/connect timeout increases to 30s, fixes to prevent streamable log leaks, and other reliability fixes.
- Miscellaneous: fixed incidents and notification issues (missing Slack notifications for workflow-generated approval tasks, incorrect user/team URLs in notifications), improved Okta public key URL handling, fixed DBT and Snowflake integration issues, and numerous other cross-area bug fixes.

### Improvements

- Custom Workflows & UI: full implementation and UI revamp for Custom Workflows, Knowledge Center and Overview improvements, domain & data-product field support, project/explorer card enhancements, domain tree view, pipeline view node/edge support, and multiple UI styling and layout improvements across the app.
- AskCollate & AI experience: AskCollate UI and chat enhancements, CAIP-related pipe updates, agent improvements, AskCollate Slack integration and chat/profile components for messages.
- SQL & Query Runner enhancements: saved-query per user, improved userAuthConfig response for UI, improved logging, SELECT-only enforcement improvements, and performance/pagination improvements for Snowflake usage/lineage.
- Connectors & exporters: added or improved connectors and exporters including AWS Kinesis Firehose, BigQuery exporter, Collibra connector, Hex dashboard connector, Kafka Connect Confluent Cloud support, ADLS unstructured containers, Collibra and Hex integrations, and PowerBI improvements (custom API URL, databricks lineage parsing).
- Embeddings & vector search: DJL local embeddings support, efficient k-NN filtering, increased neighbor limits, soft/hard delete handling, embedding model tracking, and improved vector index validation.
- Data Observability & Data Quality as Code: new Data Quality as Code APIs, DataFrame/TestRunner improvements, support for dimension-level DQ tests and a wide set of dimensionality validators (mean, median, min/max, sum, stddev, regex, not-null, uniqueness, etc.), plus UI updates for dimensionality analysis and test result exploration.
- Notifications & templates: enhanced notification templates with rich formatting, Handlebars helpers, template preview and test send, transformers, and permission controls for templates; notification template UI added.
- Performance & reliability: ingestion log streaming & caching improvements (streaming + caching for downloads), Redis added as an optional cache, increased socket/connect timeouts to 30s, improved handling for streamable logs, and general performance tweaks.
- Search & indexing improvements: Search reindex enhancements (selective entity reindex), improved stemmer language support for OpenSearch, unified ES/OS client API with separate index management, and other search reliability improvements.
- Observability & tooling: added workflow resource utilisation metrics to aid troubleshooting, improved metrics page docs and messaging, and better logging and error messages across services.
- Miscellaneous improvements: Impersonation by bots, bulk update APIs for data assets, selective entity reindex for passed entity refs, added support for classification tags in dbt meta field, Kafka lineage support in Databricks pipelines, Hex & Collibra connectors, improved Tableau logging, and other end-user enhancements.

**Full Changelog**: [link](https://github.com/open-metadata/OpenMetadata/compare/1.10.14-release...1.11.0-release)
